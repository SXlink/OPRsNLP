# OPRsNLP: Анализ отзывов о медицинских организациях Санкт-Петербурга

![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)
![License](https://img.shields.io/badge/License-MIT-yellow.svg)

Сравнение результатов автоматизированного категориального и тонального анализа отзывов с официальными показателями независимой оценки качества медицинских услуг (НОК) Комитета по здравоохранению Санкт-Петербурга (Министерство здравоохранения РФ, 2024).
> **Обратите внимание:** обучающая выборка и обученная модель предоставляются отдельно по запросу, однако повторить исследование можно на своих данных с актуализацией отзывов. Для обучающей выборки необходимо минимум 150–200 примеров на каждый класс.

## Описание проекта

### Сбор данных

Для автоматизированного категориального и тонального анализа отзывов с помощью программы парсинга информации с веб-страниц, написанной на языке программирования Python, была сформирована выборка из **26 027 отзывов** пациентов о **120 медицинских организациях** Санкт-Петербурга, размещенных на платформах «Яндекс.Карты» и «ПроДокторов».

Обе платформы применяют собственные механизмы модерации и проверки подлинности материалов:
- **«ПроДокторов»** в рамках программы «Отзывы без обмана» описывает алгоритмы верификации, включающие защиту авторов и анонимность, что минимизирует риск фальсификаций отзывов
- **«Яндекс.Карты»** модерирует все отзывы, удаляя недостоверные и нарушающие правила, при обязательной дополнительной проверке модераторами

### Предобработка текстов

Для обеспечения качества анализа из исходного корпуса исключались:
- Тексты, написанные не на русском языке
- Полные дубликаты
- Тексты объёмом менее 15 слов (~80–100 символов)

Очистка текстов включала удаление HTML-тегов, спецсимволов и чисел. Стоп-слова в задачах классификации и тонального анализа не удалялись для сохранения контекста, учитываемого предобученными моделями.

Для токенизации и преобразования текстов в векторные представления использовалась предобученная модель **ruRoBERTa Large** от [ai-forever](https://github.com/ai-forever) через библиотеку Transformers.

### Тематическое моделирование

В рамках тематического моделирования дополнительно применялись:
- Приведение текста к нижнему регистру
- Удаление пунктуации
- Удаление стоп-слов (с использованием списка стоп-слов библиотеки NLTK)
- Лемматизация с помощью морфологического анализатора **pymorphy3**

Тематическое моделирование текстов отзывов проведено при помощи библиотеки **BERTopic**:
1. Снижение размерности эмбеддингов методом **UMAP**
2. Выделение плотных кластеров и маркировка «шума» алгоритмом **HDBSCAN**
3. Выделение ключевых терминов для каждого кластера методом **c-TF-IDF**

Выбор трансформерных моделей и BERTopic обусловлен их способностью формировать контекстуализированные эмбеддинги, устойчивые к разговорной лексике и орфографическим вариациям, и моделировать дальние зависимости в тексте посредством механизма самовнимания (self-attention).

### Классификация аспектов качества

Классификация аспектов качества услуг выполнена с применением модели **ruRoBERTa Large** (Zmitrovich и др., 2024), дообученной на рандомизированной выборке (5 020 размеченных отзывов).

Разметка корпуса выполнялась методом селективного кодирования по когнитивным и эмоциональным аспектам. Каждому фрагменту текста присваивалась метка «позитивный» или «негативный» в зависимости от того, как в отзыве упоминался соответствующий аспект качества.

**Характеристики модели:**
- Обучение: 5 эпох
- Macro F1-score: ~0.8053 на тестовой выборке
- Порог вероятности для принятия меток: > 0.7
- Балансировка классов: взвешенная функция потерь `BCEWithLogitsLoss` с параметром `pos_weight`, где веса рассчитываются как отношение негативных примеров к позитивным для каждого класса

### Тональный анализ

Тональный анализ реализован с использованием **ruRoBERTa Large RuGoEmotion** от [fyaronskiy](https://huggingface.co/fyaronskiy), позволяющей классифицировать отзывы по широкому спектру эмоций (позитивные, негативные, нейтральные) и фиксировать лексико-семантические оттенки эмоциональных реакций.

### Валидация результатов

Для внешней валидизации проведена оценка согласованности результатов с данными опросного исследования оценки качества медицинских услуг Комитета по здравоохранению Санкт-Петербурга (НОК СПб, 2023–2024).

**Методология расчёта интегрального балла:**
- Каждому отзыву присваивался балл по шкале 1–5
- Исходный балл 5 уменьшался на единицу за каждую выявленную негативную категорию
- Отсутствие упоминания негативного аспекта трактовалось как соответствие ожиданиям

Для сопоставления показателей использовался **ранговый коэффициент корреляции Спирмена**.

## Структура репозитория

```
OPRsNLP/
├── Parsing.ipynb           # Парсинг отзывов с платформ
├── FineTunning.ipynb       # Дообучение классификатора и Классификация аспектов качества
├── OpenCoding.ipynb        # Визуализация открытого кодирования
├── Themes.ipynb            # Тематическое моделирование
├── src/                    # Исходные данные
│   ├── Организации.xlsx    # Список медицинских организаций
│   └── Обучающая выборка.xlsx  # Размеченные данные для обучения
└── results/                # Результаты анализа
```

## Используемые технологии

- **Python 3.x**
- **Transformers** — работа с предобученными моделями
- **BERTopic** — тематическое моделирование
- **UMAP** — снижение размерности
- **HDBSCAN** — кластеризация
- **pymorphy3** — морфологический анализ русского языка
- **NLTK** — обработка естественного языка
- **pandas**, **numpy** — обработка данных
- **matplotlib**, **seaborn** — визуализация

## Ссылки

- [ruRoBERTa Large (ai-forever)](https://huggingface.co/ai-forever/ruRoberta-large)
- [RuGoEmotion (fyaronskiy)](https://huggingface.co/fyaronskiy/ruRoberta-large-ru-go-emotions)
- [BERTopic Documentation](https://maartengr.github.io/BERTopic/)

## Литература

- Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.
- Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., ... & Stoyanov, V. (2019). RoBERTa: A Robustly Optimized BERT Pretraining Approach.
- Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention Is All You Need.
- Zmitrovich, D., Abramov, A., Kalmykov, A., Tikhonova, M., Taktasheva, E., Astafurov, D., ... & Fenogenova, A. (2024). A Family of Pretrained Transformer Language Models for Russian.

## Лицензия

MIT License
